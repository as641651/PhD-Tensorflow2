{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Talk Benjamin\n",
    "\n",
    "My PhD thesis is about performance modelling, which revolves around predicting the execution times of an algorithm.\n",
    "\n",
    "Explain \"Algorithm\" and probably define some terms that will bridge the gap between computer science and math --  my algorithm will be a psuedo code of a computer program which will compute certain mathematical equation. Suppose you have some mathematical equation (AB)C, I sort of decompose this mathmatical equation into certain standard operations.\n",
    "\n",
    "```c++\n",
    "ABC(){\n",
    "    // (AB)C\n",
    "    T = AB\n",
    "    D = TC     \n",
    "}\n",
    "```\n",
    "\n",
    "Usually there are specilized computer porgrams that compute these standard operations and they are called kernels. \n",
    "\n",
    "```c++\n",
    "void Algorithm(){\n",
    "    // (AB)C\n",
    "    dgemm(A,B,200x200,200x500,T) \n",
    "    dgemm(T,C,200x500,500x100,D) \n",
    "} \n",
    "```\n",
    "\n",
    "So from our perspective, an algorithm is a sequence of kernel call\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "My target is give an estimate of performance (eg. execution time) of this algorithm\n",
    "\n",
    "But the execution time non deterministic because of certain factors like CPU temperature, where the data is present in memory. So when you repeat an algorithm many times, what you observe is a distribution. \n",
    "\n",
    "The statistical dispersion of the execution time depends on the amount of work needed for that computation. This is a graph showing measurements of matrix multiplication for different matrix size. You can see that when the matrix size is small the spread is larger. The color dots are the 15th and 85th quantile - that is I repeat each matrix multiplation 100 times, sort them according to execution time and these dots are 15th and 85th element in that sort.\n",
    "\n",
    "<img src=\"pics/quantile_reg.png\" />\n",
    "\n",
    "So we think it's better if instead of summarizing execution time as a single number like mean or median, we ll provide an interval, ie an estimate which says execution time will lie between certain range for 70% of the time\n",
    "\n",
    "So we want to fit quantiles or use a quantile loss which assigns different penalities for over and under estimation. \n",
    "\n",
    "$L_q(y, \\mathbf{f}) = \\sum_{i=y_i < \\mathbf{f}_i} (1-q)|y_i - \\mathbf{f}_i | + \\sum_{i=y_i \\ge \\mathbf{f}_i} (q)|y_i - \\mathbf{f}_i | $\n",
    "\n",
    "<img src=\"pics/quantile_loss.png\" />\n",
    "\n",
    "What I show in this graph is for a single kernel, but in reality an alogirthm is composed of a sequence of kernel calls and I have to add up estimates of individual kernels. In most cases simple adding might work, but it does not take into effect caching and there might be certain underestimation because of this.\n",
    "\n",
    "```c++\n",
    "void Algorithm(){\n",
    "    // (AB)C\n",
    "    dgemm(A,B,200x200,200x500,B) ---> Qa_1,Qa_2,Qa_3\n",
    "    dgemm(B,C,200x500,500x100,D) ---> Qb_1,Qb_2,Qb_3\n",
    "} ---> Qfoo_1, Qfoo_2, Qfoo_3\n",
    "```\n",
    "\n",
    "Need for experiment design.\n",
    "\n",
    "> The graph is going to vary from machine to machine. It also needs to be trained separately for each kernel\n",
    "\n",
    "> We want to do some experiment design - minimize the number of measurements and at the same time do not compramise on the quality of solutions. Especially the measurements towards the end of the graph takes more time.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain about predicting execution time of an algorithm\n",
    "\n",
    "Non deterministic nature of execution time\n",
    "\n",
    "Nature of dispersion \n",
    "\n",
    "> As far as the CPU is concerned, the executiomn time = time taken for memory access (varies because of data locality) + time taken to compute (varies because of temperature and turbo boost and resource availablilty)\n",
    "\n",
    "Explain the performance graph\n",
    "\n",
    "Need for quantile regression\n",
    "\n",
    "> Eg, say we do a mat mat multiplication --> We need an estimate which the execution time will between certain range for 70% of the time\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Experiment design when the kernels are not known? I do not know anything about the kernels and all I can observe is the execution time.\n",
    "\n",
    "> Our team is developing a linear algebra compiler which generates 100s of alternative algorithms to compute same equation. \n",
    "\n",
    "> Paolo's example of comparing two algorithms -- We want to compare Alg A and Alg B. Assume we have a budget of making 15 repetitions of each algorithm. What people usually do is find min of 15 reps and compare just one min and take decision. Is it a better experiment design to do 5 repetition and compare min but repeat this 3 times. Here we still do 15 repetitions but compare 4 mins \n",
    "\n",
    "> Is minA(15 reps) > minB(15 reps)\n",
    "\n",
    "> {minA(5 reps) > minB( 5 reps)} + {minA(5 reps) > minB( 5 reps)} + {minA(5 reps) > minB( 5 reps)} / 3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
