{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Talk:\n",
    "\n",
    "Play with estimator\n",
    "\n",
    "show the abstract matchain code\n",
    "\n",
    "Talk about over estimation, difficulty with visualizing them and decide about evaluvation benchmarks\n",
    "\n",
    "Talk about abstract problem - discuss about dimensions and miniminzing f / params\n",
    "\n",
    "> Loss function takes in a ground truth and its estimates and quantifies the loss as some real number\n",
    "\n",
    "> Ground truth are all the mrasurement points and we specify the estimation as a function. We do not want to specify dimension of f, as it can depend on the problem and abstracting it at this point might confuse the reader\n",
    "\n",
    "> One of the choice of loss function is Least square loss and I think it worth pointing out the difference between the least square problem and having this loss function as least square. If f is some linear function, then we can come up with a closed form solution for parameters phi which can be solution to some linear system\n",
    "\n",
    "> But when I say least square problem it is with a broader scope - not only restricting to those function which have a nice closed solution but also more general function - neural networks\n",
    "\n",
    "> When closed form solution is not possible we go for itertive solution\n",
    "\n",
    "> Update of parameter and based on how it is driven to the optimum we have different optimization methods\n",
    "\n",
    "> Talk about computing the gradient - automatic differentiation and how it has changed our lives - inception of success in machine learning - now we can autimatically learn this function f \n",
    "\n",
    "> Talk about difference in minimizing over f vs minimizing over parameters over f\n",
    "\n",
    "> Talk about how we can acomodate loss functions which are not differentaible. A decade before, we would have needed someone with advanced math degree to differentaite this function - now we can test novel loss functions - quantile loss is one of them. - explain quantile loss\n",
    "\n",
    "> Talk about independent and joint quantile regression - we can do more fancy things like compose losses and create a new loss\n",
    "\n",
    "\n",
    "\n",
    "Talk about having benjamin as second supervisor - show his expertise in image processing and computer vision, \n",
    "\n",
    "Talk about keeping PhD thesis title as ML based performance modelling and avoid calling them statistical approaches as ML is more encompassing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feedback\n",
    "\n",
    "> Experiment design --> Henrik\n",
    "\n",
    "> Meeting with Benjamin\n",
    "\n",
    "> The parameter set $\\Phi$ was not clear, I should  have an example to explain how it as been defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
